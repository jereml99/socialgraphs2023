{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pywikibot\n",
    "\n",
    "def get_subcategories_from_category(category_name):\n",
    "    site = pywikibot.Site('en', 'wikipedia')\n",
    "    cat = pywikibot.Category(site, category_name)\n",
    "    pages = list(cat.subcategories())\n",
    "    return [page.title() for page in pages]\n",
    "\n",
    "def get_games_from_category(category_name):\n",
    "    site = pywikibot.Site('en', 'wikipedia')\n",
    "    cat = pywikibot.Category(site, category_name)\n",
    "    pages = list(cat.articles())\n",
    "    return pages\n",
    "\n",
    "def save_to_json(page, subcategory):\n",
    "    data = {\n",
    "        \"text\": page.text,\n",
    "        \"categorie\": subcategory,\n",
    "        \"title\": page.title(),\n",
    "        \"url\": page.full_url(),\n",
    "    }\n",
    "\n",
    "    directory = os.path.join('./data', subcategory.split(':')[-1])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    filepath = os.path.join(directory, page.title().replace('/', '_') + '.json')\n",
    "    with open(filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Start with the base category\n",
    "category_name = 'Category:Video games by year'\n",
    "game_categories = get_subcategories_from_category(category_name)\n",
    "\n",
    "# Go through each sub-category to get the game pages\n",
    "for game_category in game_categories:\n",
    "    # Make sure we only look at the relevant categories (avoiding 'by decade' etc.)\n",
    "    if 'video games' in game_category.lower():\n",
    "        game_pages = get_games_from_category(game_category)\n",
    "        for page in game_pages:\n",
    "            save_to_json(page, game_category.split(':')[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_all_page_names_from_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data['title']\n",
    "\n",
    "\n",
    "\n",
    "base_directory = 'project/data'\n",
    "all_page_names = []\n",
    "\n",
    "# Step 1: Get all page names\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(get_all_page_names_from_json, os.path.join(base_directory, subcategory, json_file))\n",
    "                for subcategory in os.listdir(base_directory)\n",
    "                for json_file in os.listdir(os.path.join(base_directory, subcategory))]\n",
    "    \n",
    "    for future in tqdm(futures, desc=\"Fetching Page Names\", unit=\"files\"):\n",
    "        all_page_names.append(future.result())\n",
    "\n",
    "# Save all page names to CSV\n",
    "with open('all_page_names.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Page Names'])\n",
    "    for page_name in all_page_names:\n",
    "        writer.writerow([page_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_outlinks(text):\n",
    "    \"\"\"Extract Wikipedia outlinks (wiki links) from the text.\"\"\"\n",
    "    links = re.findall(r'\\[\\[(?:[^|\\]]*\\|)?([^\\]]+)\\]\\]', text)\n",
    "    return [link.split('#')[0] for link in links]\n",
    "\n",
    "def extract_categories(text):\n",
    "    \"\"\"Extract categories from the text.\"\"\"\n",
    "    return re.findall(r'\\[\\[Category:(.*?)\\]\\]', text)\n",
    "\n",
    "def get_all_page_names_from_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data['title']\n",
    "\n",
    "# Generic function to update JSON files\n",
    "def update_json(filepath, update_func, *args):\n",
    "    with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    update_func(data, *args)  # Call the provided update function\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def update_json_with_country_of_development(data):\n",
    "    # Extract countries of development based on the categories\n",
    "    countries_of_development = []\n",
    "    for category in data.get('categories', []):\n",
    "        match = re.match(r'Video games developed in (.+)', category)\n",
    "        if match:\n",
    "            country = match.group(1)\n",
    "            countries_of_development.append(country)\n",
    "    \n",
    "    # If no countries are found, set to [\"other\"]\n",
    "    data['country of development'] = countries_of_development if countries_of_development else [\"other\"]\n",
    "    \n",
    "                  \n",
    "# Specific update methods\n",
    "def filter_outlinks(data, all_page_names):\n",
    "    data['outpages'] = [link for link in extract_outlinks(data['text']) if link in all_page_names]\n",
    "\n",
    "def add_categories(data):\n",
    "    data['categories'] = extract_categories(data['text'])\n",
    "\n",
    "\n",
    "def integration_test_on_one_file(base_directory, all_page_names):\n",
    "    test_subcategory = os.listdir(base_directory)[0]\n",
    "    test_file = os.listdir(os.path.join(base_directory, test_subcategory))[0]\n",
    "    test_filepath = os.path.join(base_directory, test_subcategory, test_file)\n",
    "    \n",
    "    print(\"Running integration test on:\", test_filepath)\n",
    "    update_json(test_filepath, filter_outlinks, all_page_names)\n",
    "    update_json(test_filepath, add_categories)\n",
    "    print(\"Test complete. Please verify\", test_filepath)\n",
    "\n",
    "\n",
    "def load_csv_as_list(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # Skip the header row if your CSV has one\n",
    "        next(reader, None)\n",
    "        return list(reader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_directory = 'project/data'\n",
    "\n",
    "# Step 2: Update JSON files with filtered outlinks and categories\n",
    "json_files = [os.path.join(base_directory, subcategory, json_file)\n",
    "                for subcategory in os.listdir(base_directory)\n",
    "                for json_file in os.listdir(os.path.join(base_directory, subcategory))]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(lambda x: update_json(x, filter_outlinks, all_page_names), json_files), \n",
    "              total=len(json_files), desc=\"Filtering Outlinks\", unit=\"files\"))\n",
    "    list(tqdm(executor.map(lambda x: update_json(x, add_categories), json_files), \n",
    "              total=len(json_files), desc=\"Adding Categories\", unit=\"files\"))\n",
    "    list(tqdm(executor.map(lambda x: update_json(x, update_json_with_country_of_development), json_files), \n",
    "        total=len(json_files), desc=\"Adding cantry of development\", unit=\"files\"))\n",
    "\n",
    "# # Run integration test\n",
    "integration_test_on_one_file(base_directory, all_page_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import mwparserfromhell\n",
    "\n",
    "def parse_wiki_text_to_sections(wiki_text):\n",
    "    # Parse the text with mwparserfromhell\n",
    "    wikicode = mwparserfromhell.parse(wiki_text, skip_style_tags=True)\n",
    "    \n",
    "    sections_dict = {}\n",
    "    current_section = 'introduction'\n",
    "    sections_dict[current_section] = ''  # Initialize the intro section\n",
    "\n",
    "    # Iterate through the parsed wiki code\n",
    "    for node in wikicode.nodes:\n",
    "        if isinstance(node, mwparserfromhell.nodes.heading.Heading):\n",
    "            # When we find a heading, set the current section to the heading's title\n",
    "            current_section = str(node.title).strip().lower()\n",
    "            sections_dict[current_section] = ''\n",
    "        else:\n",
    "            # Otherwise, append the text of this node to the current section\n",
    "            sections_dict[current_section] += str(node)\n",
    "\n",
    "    # Clean up text for each section\n",
    "    for section, text in sections_dict.items():\n",
    "        # Remove references and other unwanted parts\n",
    "        text = mwparserfromhell.parse(text).strip_code()\n",
    "        sections_dict[section] = text\n",
    "\n",
    "    return sections_dict\n",
    "\n",
    "def load_json_data_to_dict(base_directory):\n",
    "    all_data = {}  # Dictionary to hold all data from JSON files, keyed by the title\n",
    "    for subdir, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                filepath = os.path.join(subdir, file)\n",
    "                with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    data[\"text\"] = parse_wiki_text_to_sections(data[\"text\"])\n",
    "                    # Use the title as the key for the dictionary\n",
    "                    all_data[data['title']] = data\n",
    "    return all_data\n",
    "\n",
    "# Replace with the actual path to your data directory\n",
    "base_directory = './data'  # Update with the actual path\n",
    "data_file = 'game_data.json'\n",
    "\n",
    "if os.path.exists(data_file):\n",
    "    with open(data_file) as f:\n",
    "        json_data_dict = json.load(f)\n",
    "else:\n",
    "    json_data_dict = load_json_data_to_dict(base_directory)\n",
    "    with open('game_data.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_data_dict, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_file = '../game_data.json'\n",
    "\n",
    "\n",
    "with open(data_file, encoding=\"utf-8\") as f:\n",
    "    game_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_histogram(data, title, xlabel, ylabel, color='tab:blue', figuresize=(10, 6)):\n",
    "    labels = list(data.keys())\n",
    "    values = list(data.values())\n",
    "\n",
    "    n_bars = len(labels)\n",
    "    figure_width = max(labels) - min(labels)\n",
    "\n",
    "    bar_width = figure_width / (1.5 * n_bars)\n",
    "\n",
    "    plt.figure(figsize=figuresize)\n",
    "    plt.bar(labels, values, color=color, edgecolor='black', width=bar_width)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# Function to get sentiment scores\n",
    "weight_neg = 0.7\n",
    "weight_compound = 1-  weight_neg \n",
    "\n",
    "def normalize_score(score, old_min, old_max, new_min, new_max):\n",
    "    return ((score - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "def get_violence_score(neg_score, pos_score, weight_neg, weight_compound):\n",
    "    # Normalize 'neg' score from [0, 1] to [-1, 1]\n",
    "    normalized_neg_score = normalize_score(neg_score, 0, 0.35, 0, 1)\n",
    "    normalized_pos_score = normalize_score(pos_score, 0, 0.3, 0, 1)\n",
    "    \n",
    "    # Calculate the weighted score\n",
    "    violence_score = normalized_neg_score * weight_neg - normalized_pos_score * weight_compound\n",
    "    \n",
    "    return violence_score\n",
    "\n",
    "\n",
    "def get_sentiment_scores(data, subsections = [\"gameplay\"]):\n",
    "    sentiment_scores = {}\n",
    "    for title, content in data.items():\n",
    "        text = \"\\n\".join([content['text'][subsection] for subsection in subsections if subsection in content[\"text\"]])\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        sentiment[\"violence\"] = get_violence_score(sentiment[\"neg\"], sentiment[\"pos\"], weight_neg, weight_compound)\n",
    "        sentiment_scores[title] = sentiment\n",
    "    return sentiment_scores\n",
    "\n",
    "# Function to create a histogram of sentiment scores\n",
    "def create_sentiment_histograms(sentiment_data, sentiments_to_plot=None):\n",
    "    # Default to all sentiment types if none are specified\n",
    "    if sentiments_to_plot is None:\n",
    "        sentiments_to_plot = ['pos', 'neu', 'neg', 'compound']\n",
    "\n",
    "    # Determine the number of plots\n",
    "    num_plots = len(sentiments_to_plot)\n",
    "    cols = 2  # We prefer a 2-column layout\n",
    "    rows = (num_plots + 1) // cols  # Calculate rows needed\n",
    "\n",
    "    # Setting up the figure for multiple subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if rows > 1:\n",
    "        axes = axes.flatten()  # Flatten if we have more than one row\n",
    "    else:\n",
    "        axes = [axes]  # Wrap in list if only one row (i.e., 1 or 2 plots)\n",
    "\n",
    "    fig.suptitle('Sentiment Analysis Histograms')\n",
    "\n",
    "    # Plotting each requested sentiment\n",
    "    for i, sentiment in enumerate(sentiments_to_plot):\n",
    "        scores = [details[sentiment] for details in sentiment_data.values()]\n",
    "        ax = axes[i]\n",
    "        ax.hist(scores, bins=200, color='tab:blue', edgecolor='black')\n",
    "        ax.set_title(f'{sentiment.capitalize()} Sentiment Score')\n",
    "        ax.set_xlabel('Sentiment Score')\n",
    "        ax.set_ylabel('Number of Games')\n",
    "\n",
    "    # Turn off any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def print_top_bottom_sentiment_games(sentiment_scores, key='compound', n = 10):\n",
    "    # Sorting the games based on the compound sentiment score\n",
    "    sorted_games = sorted(sentiment_scores.items(), key=lambda x: x[1][key])\n",
    "    \n",
    "\n",
    "    # Printing the 10 most negative games\n",
    "    print(f\"{n} Most Negative Games in terms of {key} sentiment:\")\n",
    "    for game, score in sorted_games[:n]:\n",
    "        print(f\"{game}: {score}\")\n",
    "\n",
    "    print(\"\\n\")  # Adding a newline for better readability\n",
    "\n",
    "    # Printing the 10 most positive games\n",
    "    print(f\"{n} Most Positive Games in terms of {key} sentiment:\")\n",
    "    for game, score in sorted_games[-n:]:\n",
    "        print(f\"{game}: {score}\")\n",
    "\n",
    "def is_subsection_length_valid(data, subsections, min_length, max_length):\n",
    "    \"\"\"\n",
    "    Check if the length of a subsection is within the specified range.\n",
    "    \n",
    "    :param data: The data dictionary of a game\n",
    "    :param subsection: The subsection to check within the data\n",
    "    :param min_length: The minimum length of the subsection string\n",
    "    :param max_length: The maximum length of the subsection string\n",
    "    :return: True if the length is within range, False otherwise\n",
    "    \"\"\"\n",
    "    subsection_text = \"\"\n",
    "    for subsection in subsections:\n",
    "        subsection_text += data.get('text', {}).get(subsection, \"\") + \"\\n\"\n",
    "    word_count = len(subsection_text.split())\n",
    "    return min_length <= word_count <= max_length\n",
    "\n",
    "def count_long_subsections(game_data, subsection='gameplay', min_length=0, max_length=float('inf')):\n",
    "    \"\"\"\n",
    "    Counts the number of entries in the game_data dictionary that have a specified subsection\n",
    "    with a string length within the given range.\n",
    "    \n",
    "    :param game_data: Dictionary containing game information\n",
    "    :param subsection: The subsection to look for within the entries (default is 'gameplay')\n",
    "    :param min_length: The minimum length of the subsection string to count (default is 0)\n",
    "    :param max_length: The maximum length of the subsection string to count (default is infinity)\n",
    "    :return: The count of entries with the subsection string length within the specified range\n",
    "    \"\"\"\n",
    "    return sum(is_subsection_length_valid(data, subsection, min_length, max_length) for _, data in game_data.items())\n",
    "\n",
    "def filter_entries_by_length(game_data, subsections=['gameplay'], min_length=0, max_length=float('inf')):\n",
    "    \"\"\"\n",
    "    Creates a dictionary with only the entries from game_data that have a specified subsection\n",
    "    with a string length within the given range.\n",
    "    \n",
    "    :param game_data: Dictionary containing game information\n",
    "    :param subsection: The subsection to look for within the entries (default is 'gameplay')\n",
    "    :param min_length: The minimum length of the subsection string to filter by (default is 0)\n",
    "    :param max_length: The maximum length of the subsection string to filter by (default is infinity)\n",
    "    :return: A new dictionary with filtered entries\n",
    "    \"\"\"\n",
    "    return {game: data for game, data in game_data.items() if is_subsection_length_valid(data, subsections, min_length, max_length)}\n",
    "\n",
    "\n",
    "def get_or_create_value(function_to_apply, file_path):\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the value from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            value = json.load(file)\n",
    "    else:\n",
    "        # Call the function to create the value\n",
    "        value = function_to_apply()\n",
    "        # Save the value to the file\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(value, file)\n",
    "    \n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sentiment dictionart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrains = {\n",
    "     \"min_length\": 100,\n",
    "    \"max_length\": 10000\n",
    "}\n",
    "subsections = ['gameplay','plot','story','synopsis','plot and gameplay','plot and gameplay']\n",
    "\n",
    "filtered_by_length = filter_entries_by_length(game_data, subsections=subsections, **constrains)\n",
    "sentiment_filtered_by_length = get_or_create_value( lambda: get_sentiment_scores(filtered_by_length, subsections), file_path= \"filtered_gameplay_sentiment.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violence = [sentiment[\"violence\"] for game, sentiment in sentiment_filtered_by_length.items()]\n",
    "for game, sentiment in sentiment_filtered_by_length.items():\n",
    "    sentiment[\"violence\"] = normalize_score(sentiment[\"violence\"], min(violence), max(violence), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sentiment_histograms(sentiment_data=sentiment_filtered_by_length, sentiments_to_plot= ['pos', 'neu', 'neg', 'compound', \"violence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data_with_vader_sentiment_on_gameplay = get_or_create_value(lambda: {\n",
    "    game: {**data, 'sentiment': sentiment_filtered_by_length[game]}\n",
    "    for game, data in filtered_by_length.items() }, \"game_data_with_vader_sentiment.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_violence_scores(data, most_violent, least_violent):\n",
    "    # Initialize dictionaries to hold the scores for the most and least violent games\n",
    "    most_violences = {'neg': [], 'neu': [], 'pos': [], 'compound': [], \"violence\": []}\n",
    "    least_violences = {'neg': [], 'neu': [], 'pos': [], 'compound': [], \"violence\": []}\n",
    "    \n",
    "    # Helper function to calculate average of a list\n",
    "    def average(lst):\n",
    "        return sum(lst) / len(lst) if lst else 0\n",
    "    \n",
    "    # Extract scores for each game in the most and least violent games lists\n",
    "    for game in most_violent:\n",
    "        if game in data:\n",
    "            most_violences['neg'].append(data[game]['neg'])\n",
    "            most_violences['neu'].append(data[game]['neu'])\n",
    "            most_violences['pos'].append(data[game]['pos'])\n",
    "            most_violences['compound'].append(data[game]['compound'])\n",
    "            most_violences['violence'].append(data[game]['violence'])\n",
    "    \n",
    "    for game in least_violent:\n",
    "        if game in data:\n",
    "            least_violences['neg'].append(data[game]['neg'])\n",
    "            least_violences['neu'].append(data[game]['neu'])\n",
    "            least_violences['pos'].append(data[game]['pos'])\n",
    "            least_violences['compound'].append(data[game]['compound'])\n",
    "            least_violences['violence'].append(data[game]['violence'])\n",
    "    \n",
    "    # Calculate averages for both groups\n",
    "    averages = {\n",
    "        'Score': ['neg', 'neu', 'pos', 'compound', \"violence\"],\n",
    "        'Average Most Violent': [\n",
    "            average(most_violences['neg']),\n",
    "            average(most_violences['neu']),\n",
    "            average(most_violences['pos']),\n",
    "            average(most_violences['compound']),\n",
    "            average(most_violences['violence'])\n",
    "        ],\n",
    "        'Average Least Violent': [\n",
    "            average(least_violences['neg']),\n",
    "            average(least_violences['neu']),\n",
    "            average(least_violences['pos']),\n",
    "            average(least_violences['compound']),\n",
    "            average(least_violences['violence'])\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame to display the table\n",
    "    df = pd.DataFrame(averages)\n",
    "    \n",
    "    # Calculate and add a column for the difference between the most and least violent scores\n",
    "    df['Difference (Most - Least)'] = df['Average Most Violent'] - df['Average Least Violent']\n",
    "    \n",
    "    return df\n",
    "\n",
    "most_violent = [\"Doom (2016 video game)\", \"Grand Theft Auto V\", \"Mortal Kombat (1992 video game)\", \"God of War (2005 video game)\", \"Manhunt (video game)\", \"Gears of War (video game)\", \"Call of Duty 4: Modern Warfare\", \"Dead Space (2008 video game)\", \"Resident Evil (1996 video game)\", \"Hotline Miami\"]\n",
    "least_violent = [\"Animal Crossing: New Horizons\", \"Stardew Valley\", \"The Sims 4\", \"Minecraft\", \"Tetris\", \"Monument Valley (video game)\", \"Super Mario Odyssey\", \"Journey (2012 video game)\", \"Katamari Damacy\", \"Fez (video game)\"]\n",
    "compare_violence_scores(sentiment_filtered_by_length, most_violent, least_violent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = get_or_create_value(lambda: {}, \"game_data_with_vader_sentiment.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting production year from category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for game, data in game_data.items():\n",
    "    match = re.match(r'^\\d+', data[\"categorie\"])\n",
    "    if match:\n",
    "        data[\"year\"] = int(match.group())\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "    data.pop(\"categorie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def calculate_sentiment_statistics(game_data: dict, sentiment_key: str):\n",
    "    \"\"\"\n",
    "    Calculate the average and standard deviation of specified sentiment scores per year from a dataset\n",
    "    where each entry is a dictionary containing a year and a nested 'sentiment' dictionary with sentiment scores.\n",
    "\n",
    "    :param game_data: Dictionary of dictionaries with 'year' and nested 'sentiment' dictionary.\n",
    "    :param sentiment_key: Key for the sentiment score to be calculated (e.g., 'violence', 'compound').\n",
    "    :return: Two dictionaries with years as keys and average scores and standard deviations as values.\n",
    "    \"\"\"\n",
    "    sentiment_sum_per_year = defaultdict(float)\n",
    "    count_per_year = defaultdict(int)\n",
    "\n",
    "    # Sum sentiment scores and count entries for each year\n",
    "    for game in game_data.values():\n",
    "        try:\n",
    "            year = game['year']\n",
    "            sentiment_score = game[\"sentiment\"][sentiment_key]\n",
    "            sentiment_sum_per_year[year] += sentiment_score\n",
    "            count_per_year[year] += 1\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in data: {e}\")\n",
    "\n",
    "    # Calculate the average sentiment score for each year\n",
    "    average_sentiment_per_year = {year: sentiment_sum_per_year[year] / count_per_year[year]\n",
    "                                  for year in sentiment_sum_per_year}\n",
    "\n",
    "    # Calculate standard deviation for each year\n",
    "    std_dev_per_year = {year: np.std([game[\"sentiment\"][sentiment_key] \n",
    "                                      for game in game_data.values() if game['year'] == year])\n",
    "                        for year in count_per_year}\n",
    "\n",
    "    return average_sentiment_per_year, std_dev_per_year\n",
    "\n",
    "\n",
    "\n",
    "def plot_violence_statistics(average_violence, std_deviation):\n",
    "    \"\"\"\n",
    "    Plot the violence statistics.\n",
    "\n",
    "    :param average_violence: Dictionary of average violence scores per year.\n",
    "    :param std_deviation: Dictionary of standard deviations per year.\n",
    "    \"\"\"\n",
    "    # Sort the data by year\n",
    "    sorted_years = sorted(average_violence.keys())\n",
    "    average_violences = [average_violence[year] for year in sorted_years]\n",
    "    std_devs = [std_deviation[year] for year in sorted_years]\n",
    "\n",
    "    # Plotting with enhancements\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot average violence scores with error bars\n",
    "    plt.errorbar(sorted_years, average_violences, yerr=std_devs, fmt='-o',\n",
    "                 label='Average with Std Dev', color='blue')\n",
    "\n",
    "    # Fit and plot a trend line\n",
    "    z = np.polyfit(sorted_years, average_violences, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(sorted_years, p(sorted_years), \"r--\", label='Trend Line')\n",
    "\n",
    "    # Labels and title\n",
    "    plt.title('Violence Scores Over the Years with Trend and Variability')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Violence Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "average_violence, std_deviation = calculate_sentiment_statistics(game_data, \"violence\")\n",
    "plot_violence_statistics(average_violence, std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Assuming game_data is a dictionary where the key is the game name and the value is another dictionary\n",
    "# that includes 'country of development' and 'violence' score within a 'sentiment' sub-dictionary.\n",
    "\n",
    "# Step 1: Extract relevant data\n",
    "country_violence_data = defaultdict(list)\n",
    "for game, details in game_data.items():\n",
    "    # We assume 'violence' scores are within a 'sentiment' sub-dictionary\n",
    "    violence_score = details[\"sentiment\"][\"violence\"]\n",
    "    for country in details[\"country of development\"]:\n",
    "        country_violence_data[country].append(violence_score)\n",
    "\n",
    "# Step 2: Count the number of games per country\n",
    "game_counts = Counter({country: len(scores) for country, scores in country_violence_data.items()})\n",
    "\n",
    "# Step 3: Calculate average violence score for each country with at least 100 games or is Denmark\n",
    "average_violence_scores = {}\n",
    "for country, violence_scores in country_violence_data.items():\n",
    "    if (game_counts[country] >= 100 or country == \"Denmark\") and country != \"other\":\n",
    "        average_violence_scores[country] = sum(violence_scores) / len(violence_scores)\n",
    "\n",
    "# Step 4: Add all other countries to 'Others'\n",
    "other_violence_scores = []\n",
    "for country, violence_scores in country_violence_data.items():\n",
    "    if (game_counts[country] < 100 and country != \"Denmark\") or \"other\":\n",
    "        other_violence_scores.extend(violence_scores)\n",
    "if other_violence_scores:\n",
    "    average_violence_scores[\"Others\"] = sum(other_violence_scores) / len(other_violence_scores)\n",
    "\n",
    "\n",
    "# Calculate overall average violence score\n",
    "all_scores = [score for scores in country_violence_data.values() for score in scores]\n",
    "overall_average_violence = sum(all_scores) / len(all_scores)\n",
    "\n",
    "# Step 5: Plot the average violence scores\n",
    "countries = sorted(list(average_violence_scores.keys()), key= lambda x: average_violence_scores[x]) \n",
    "averages = [average_violence_scores[country] for country in countries]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(countries, averages)\n",
    "plt.xlabel('Country of Development')\n",
    "plt.ylabel('Average Violence Score')\n",
    "plt.title('Average Violence Score by Country of Development')\n",
    "plt.xticks(rotation=90)  # Rotate country names for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of tick-labels\n",
    "\n",
    "# Add a line for the overall average violence level\n",
    "plt.axhline(y=overall_average_violence, color='r', linestyle='-', label=f'Overall Average ({overall_average_violence:.2f})')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
