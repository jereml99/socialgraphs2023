{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scraping the Wikipedia data we used pywikibot, which is a Python library specifically built for this.\n",
    "1. Loop through subcategories from a base category \"Video games by year\"\n",
    "2. Filter the selected subcategories to exclude irrelevant pages like \"Video game franchises by year of introduction\"\n",
    "3. Loop through the articles in each subcategory\n",
    "4. Save the article text, category, title and URL as JSON nodes, one JSON-file per article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pywikibot\n",
    "\n",
    "def get_subcategories_from_category(category_name):\n",
    "    site = pywikibot.Site('en', 'wikipedia')\n",
    "    cat = pywikibot.Category(site, category_name)\n",
    "    pages = list(cat.subcategories())\n",
    "    return [page.title() for page in pages]\n",
    "\n",
    "def get_games_from_category(category_name):\n",
    "    site = pywikibot.Site('en', 'wikipedia')\n",
    "    cat = pywikibot.Category(site, category_name)\n",
    "    pages = list(cat.articles())\n",
    "    return pages\n",
    "\n",
    "def save_to_json(page, subcategory):\n",
    "    data = {\n",
    "        \"text\": page.text,\n",
    "        \"category\": subcategory,\n",
    "        \"title\": page.title(),\n",
    "        \"url\": page.full_url(),\n",
    "    }\n",
    "\n",
    "    directory = os.path.join('./data', subcategory.split(':')[-1])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    filepath = os.path.join(directory, page.title().replace('/', '_') + '.json')\n",
    "    with open(filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Start with the base category\n",
    "category_name = 'Category:Video games by year'\n",
    "game_categories = get_subcategories_from_category(category_name)\n",
    "\n",
    "# Go through each sub-category to get the game pages\n",
    "for game_category in game_categories:\n",
    "    # Make sure we only look at the relevant categories (avoiding 'by decade' etc.)\n",
    "    if 'video games' in game_category.lower():\n",
    "        game_pages = get_games_from_category(game_category)\n",
    "        for page in game_pages:\n",
    "            save_to_json(page, game_category.split(':')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we processed the downloaded data for easier usage with sentiment and network analysis.\n",
    "\n",
    "1. Loop through each downloaded file in each subfolder\n",
    "2. Append each page name to a new row in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_all_page_names_from_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data['title']\n",
    "\n",
    "base_directory = 'project/data'\n",
    "all_page_names = []\n",
    "\n",
    "# Get all page names (multithreading is used for better performance)\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(get_all_page_names_from_json, os.path.join(base_directory, subcategory, json_file))\n",
    "                for subcategory in os.listdir(base_directory)\n",
    "                for json_file in os.listdir(os.path.join(base_directory, subcategory))]\n",
    "    \n",
    "    for future in tqdm(futures, desc=\"Fetching Page Names\", unit=\"files\"):\n",
    "        all_page_names.append(future.result())\n",
    "\n",
    "# Save all page names to CSV\n",
    "with open('all_page_names.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Page Names'])\n",
    "    for page_name in all_page_names:\n",
    "        writer.writerow([page_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Iterate through all article JSONs based on the file name list in the CSV file\n",
    "4. Extract and filter out all outlinks (other articles) in the JSON\n",
    "5. Extract and insert categories as a subnode in the JSON\n",
    "6. Find the country of development, extract the name and insert it as a subnode in the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_outlinks(text):\n",
    "    links = re.findall(r'\\[\\[(?:[^|\\]]*\\|)?([^\\]]+)\\]\\]', text)\n",
    "    return [link.split('#')[0] for link in links]\n",
    "\n",
    "def extract_categories(text):\n",
    "    return re.findall(r'\\[\\[Category:(.*?)\\]\\]', text)\n",
    "\n",
    "def get_all_page_names_from_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data['title']\n",
    "\n",
    "# Generic function to update JSON files\n",
    "def update_json(filepath, update_func, *args):\n",
    "    with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    update_func(data, *args)  # Call the provided update function\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def update_json_with_country_of_development(data):\n",
    "    # Extract development based on the categories\n",
    "    countries_of_development = []\n",
    "    for category in data.get('categories', []):\n",
    "        match = re.match(r'Video games developed in (.+)', category)\n",
    "        if match:\n",
    "            country = match.group(1)\n",
    "            countries_of_development.append(country)\n",
    "    \n",
    "    # If no countries are found, set to [\"other\"]\n",
    "    data['country of development'] = countries_of_development if countries_of_development else [\"other\"]\n",
    "     \n",
    "# Specific update methods\n",
    "def filter_outlinks(data, all_page_names):\n",
    "    data['outpages'] = [link for link in extract_outlinks(data['text']) if link in all_page_names]\n",
    "\n",
    "def add_categories(data):\n",
    "    data['categories'] = extract_categories(data['text'])\n",
    "\n",
    "def load_csv_as_list(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # Skip the header row if the CSV has one\n",
    "        next(reader, None)\n",
    "        return list(reader)\n",
    "\n",
    "base_directory = 'project/data'\n",
    "\n",
    "# Update JSON files with filtered outlinks and categories\n",
    "json_files = [os.path.join(base_directory, subcategory, json_file)\n",
    "                for subcategory in os.listdir(base_directory)\n",
    "                for json_file in os.listdir(os.path.join(base_directory, subcategory))]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(lambda x: update_json(x, filter_outlinks, all_page_names), json_files), \n",
    "              total=len(json_files), desc=\"Filtering Outlinks\", unit=\"files\"))\n",
    "    list(tqdm(executor.map(lambda x: update_json(x, add_categories), json_files), \n",
    "              total=len(json_files), desc=\"Adding Categories\", unit=\"files\"))\n",
    "    list(tqdm(executor.map(lambda x: update_json(x, update_json_with_country_of_development), json_files), \n",
    "        total=len(json_files), desc=\"Adding country of development\", unit=\"files\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extracting the required sections, we used the library mwparserfromhell, which is targeted for MediaWiki-style wikis, such as Wikipedia.\n",
    "\n",
    "1. Loop through all articles\n",
    "2. Iterate through article text to find headings\n",
    "3. Save the section to a dictionary node\n",
    "4. Remove references and other formatting\n",
    "5. Save all sections per article as a JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import mwparserfromhell\n",
    "\n",
    "def parse_wiki_text_to_sections(wiki_text):\n",
    "    # Parse the text with mwparserfromhell\n",
    "    wikicode = mwparserfromhell.parse(wiki_text, skip_style_tags=True)\n",
    "    \n",
    "    sections_dict = {}\n",
    "    current_section = 'introduction'\n",
    "    sections_dict[current_section] = '' # Initialize the intro section\n",
    "\n",
    "    # Iterate through the parsed wiki code\n",
    "    for node in wikicode.nodes:\n",
    "        if isinstance(node, mwparserfromhell.nodes.heading.Heading):\n",
    "            # When we find a heading, set the current section to the heading's title\n",
    "            current_section = str(node.title).strip().lower()\n",
    "            sections_dict[current_section] = ''\n",
    "        else:\n",
    "            # Otherwise, append the text of this node to the current section\n",
    "            sections_dict[current_section] += str(node)\n",
    "\n",
    "    # Clean up text for each section\n",
    "    for section, text in sections_dict.items():\n",
    "        # Remove references and other unwanted parts\n",
    "        text = mwparserfromhell.parse(text).strip_code()\n",
    "        sections_dict[section] = text\n",
    "\n",
    "    return sections_dict\n",
    "\n",
    "def load_json_data_to_dict(base_directory):\n",
    "    all_data = {}  \n",
    "    for subdir, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                filepath = os.path.join(subdir, file)\n",
    "                with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    data[\"text\"] = parse_wiki_text_to_sections(data[\"text\"])\n",
    "                    all_data[data['title']] = data\n",
    "    return all_data\n",
    "\n",
    "base_directory = './data'\n",
    "data_file = 'game_data.json'\n",
    "\n",
    "if os.path.exists(data_file):\n",
    "    with open(data_file) as f:\n",
    "        json_data_dict = json.load(f)\n",
    "else:\n",
    "    json_data_dict = load_json_data_to_dict(base_directory)\n",
    "    with open('game_data.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_data_dict, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file containing all sections of all articles.\n",
    "data_file = '../game_data.json'\n",
    "\n",
    "with open(data_file, encoding=\"utf-8\") as f:\n",
    "    game_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram with specified parameters\n",
    "def create_histogram(data, title, xlabel, ylabel, color='tab:blue', figuresize=(10, 6)):\n",
    "    labels = list(data.keys())\n",
    "    values = list(data.values())\n",
    "\n",
    "    n_bars = len(labels)\n",
    "    figure_width = max(labels) - min(labels)\n",
    "\n",
    "    bar_width = figure_width / (1.5 * n_bars)\n",
    "\n",
    "    plt.figure(figsize=figuresize)\n",
    "    plt.bar(labels, values, color=color, edgecolor='black', width=bar_width)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "import json\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# Function to get sentiment scores\n",
    "weight_neg = 0.7\n",
    "weight_compound = 1-  weight_neg \n",
    "\n",
    "def normalize_score(score, old_min, old_max, new_min, new_max):\n",
    "    return ((score - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "# Our \"violence\" score algorithm\n",
    "def get_violence_score(neg_score, pos_score, weight_neg, weight_compound):\n",
    "    # Normalize 'neg' score from [0, 1] to [-1, 1]\n",
    "    normalized_neg_score = normalize_score(neg_score, 0, 0.35, 0, 1)\n",
    "    normalized_pos_score = normalize_score(pos_score, 0, 0.3, 0, 1)\n",
    "    \n",
    "    # Calculate the weighted score\n",
    "    violence_score = normalized_neg_score * weight_neg - normalized_pos_score * weight_compound\n",
    "    \n",
    "    return violence_score\n",
    "\n",
    "# Get Vader's sentiment scores and insert them to our violence store method\n",
    "def get_sentiment_scores(data, subsections = [\"gameplay\"]):\n",
    "    sentiment_scores = {}\n",
    "    for title, content in data.items():\n",
    "        text = \"\\n\".join([content['text'][subsection] for subsection in subsections if subsection in content[\"text\"]])\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        sentiment[\"violence\"] = get_violence_score(sentiment[\"neg\"], sentiment[\"pos\"], weight_neg, weight_compound)\n",
    "        sentiment_scores[title] = sentiment\n",
    "    return sentiment_scores\n",
    "\n",
    "# Create a histogram of sentiment scores\n",
    "def create_sentiment_histograms(sentiment_data, sentiments_to_plot=None):\n",
    "    # Default to all sentiment types if none are specified\n",
    "    if sentiments_to_plot is None:\n",
    "        sentiments_to_plot = ['pos', 'neu', 'neg', 'compound']\n",
    "\n",
    "    # Determine the number of plots\n",
    "    num_plots = len(sentiments_to_plot)\n",
    "    cols = 2  # We prefer a 2-column layout\n",
    "    rows = (num_plots + 1) // cols  # Calculate rows needed\n",
    "\n",
    "    # Set up the figure for multiple subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if rows > 1:\n",
    "        axes = axes.flatten()  # Flatten if we have more than one row\n",
    "    else:\n",
    "        axes = [axes]  # Wrap in list if only one row (i.e., 1 or 2 plots)\n",
    "\n",
    "    fig.suptitle('Sentiment Analysis Histograms')\n",
    "\n",
    "    # Plotting each requested sentiment\n",
    "    for i, sentiment in enumerate(sentiments_to_plot):\n",
    "        scores = [details[sentiment] for details in sentiment_data.values()]\n",
    "        ax = axes[i]\n",
    "        ax.hist(scores, bins=200, color='tab:blue', edgecolor='black')\n",
    "        ax.set_title(f'{sentiment.capitalize()} Sentiment Score')\n",
    "        ax.set_xlabel('Sentiment Score')\n",
    "        ax.set_ylabel('Number of Games')\n",
    "\n",
    "    # Turn off any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def print_top_bottom_sentiment_games(sentiment_scores, key='compound', n = 10):\n",
    "    # Sorting the games based on the compound sentiment score\n",
    "    sorted_games = sorted(sentiment_scores.items(), key=lambda x: x[1][key])\n",
    "    \n",
    "    # Print the 10 most negative games\n",
    "    print(f\"{n} Most Negative Games in terms of {key} sentiment:\")\n",
    "    for game, score in sorted_games[:n]:\n",
    "        print(f\"{game}: {score}\")\n",
    "\n",
    "    print(\"\\n\") # Newline\n",
    "\n",
    "    # Print the 10 most positive games\n",
    "    print(f\"{n} Most Positive Games in terms of {key} sentiment:\")\n",
    "    for game, score in sorted_games[-n:]:\n",
    "        print(f\"{game}: {score}\")\n",
    "\n",
    "# Check if the length of a subsection is within the specified range.\n",
    "def is_subsection_length_valid(data, subsections, min_length, max_length):\n",
    "    subsection_text = \"\"\n",
    "    for subsection in subsections:\n",
    "        subsection_text += data.get('text', {}).get(subsection, \"\") + \"\\n\"\n",
    "    word_count = len(subsection_text.split())\n",
    "    return min_length <= word_count <= max_length\n",
    "\n",
    "# Count the number of entries in the game_data dictionary that have a specified subsection with a string length within the given range.\n",
    "def count_long_subsections(game_data, subsection='gameplay', min_length=0, max_length=float('inf')):\n",
    "    return sum(is_subsection_length_valid(data, subsection, min_length, max_length) for _, data in game_data.items())\n",
    "\n",
    "# Create a dictionary with only the entries from game_data that have a specified subsection with a string length within the given range.\n",
    "def filter_entries_by_length(game_data, subsections=['gameplay'], min_length=0, max_length=float('inf')):\n",
    "    return {game: data for game, data in game_data.items() if is_subsection_length_valid(data, subsections, min_length, max_length)}\n",
    "\n",
    "# Save specified data as a JSON file\n",
    "def get_or_create_value(function_to_apply, file_path):\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the value from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            value = json.load(file)\n",
    "    else:\n",
    "        # Call the function to create the value\n",
    "        value = function_to_apply()\n",
    "        # Save the value to the file\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(value, file)\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sentiment dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a sentiment dictionary by looping through the predefined sections in all files, then calculating the violence score in that section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraints for filtering entries by length\n",
    "constrains = {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 10000\n",
    "}\n",
    "\n",
    "# List of subsections to filter from game_data\n",
    "subsections = ['gameplay', 'plot', 'story', 'synopsis', 'plot and gameplay', 'plot and gameplay']\n",
    "\n",
    "# Filtering entries by length based on constraints\n",
    "filtered_by_length = filter_entries_by_length(game_data, subsections=subsections, **constrains)\n",
    "\n",
    "# Getting sentiment scores for filtered entries and storing them in a JSON file\n",
    "sentiment_filtered_by_length = get_or_create_value(\n",
    "    lambda: get_sentiment_scores(filtered_by_length, subsections),\n",
    "    file_path=\"filtered_gameplay_sentiment.json\"\n",
    ")\n",
    "\n",
    "# Extracting violence scores for each game from the sentiment data\n",
    "violence = [sentiment[\"violence\"] for game, sentiment in sentiment_filtered_by_length.items()]\n",
    "\n",
    "# Normalizing violence scores between 0 and 1\n",
    "for game, sentiment in sentiment_filtered_by_length.items():\n",
    "    sentiment[\"violence\"] = normalize_score(sentiment[\"violence\"], min(violence), max(violence), 0, 1)\n",
    "\n",
    "# Creating histograms for sentiment analysis including violence\n",
    "create_sentiment_histograms(\n",
    "    sentiment_data=sentiment_filtered_by_length,\n",
    "    sentiments_to_plot=['pos', 'neu', 'neg', 'compound', \"violence\"]\n",
    ")\n",
    "\n",
    "# Adding VADER sentiment analysis results for gameplay to game_data\n",
    "game_data_with_vader_sentiment_on_gameplay = get_or_create_value(\n",
    "    lambda: {\n",
    "        game: {**data, 'sentiment': sentiment_filtered_by_length[game]}\n",
    "        for game, data in filtered_by_length.items()\n",
    "    },\n",
    "    \"game_data_with_vader_sentiment.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares different scoring algorithms with visual graphs to find out which one is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_violence_scores(data, most_violent, least_violent):\n",
    "    # Initialize dictionaries to hold the scores for the most and least violent games\n",
    "    most_violences = {'neg': [], 'neu': [], 'pos': [], 'compound': [], \"violence\": []}\n",
    "    least_violences = {'neg': [], 'neu': [], 'pos': [], 'compound': [], \"violence\": []}\n",
    "    \n",
    "    # Helper function to calculate average of a list\n",
    "    def average(lst):\n",
    "        return sum(lst) / len(lst) if lst else 0\n",
    "    \n",
    "    # Extract scores for each game in the most and least violent games lists\n",
    "    for game in most_violent:\n",
    "        if game in data:\n",
    "            most_violences['neg'].append(data[game]['neg'])\n",
    "            most_violences['neu'].append(data[game]['neu'])\n",
    "            most_violences['pos'].append(data[game]['pos'])\n",
    "            most_violences['compound'].append(data[game]['compound'])\n",
    "            most_violences['violence'].append(data[game]['violence'])\n",
    "    \n",
    "    for game in least_violent:\n",
    "        if game in data:\n",
    "            least_violences['neg'].append(data[game]['neg'])\n",
    "            least_violences['neu'].append(data[game]['neu'])\n",
    "            least_violences['pos'].append(data[game]['pos'])\n",
    "            least_violences['compound'].append(data[game]['compound'])\n",
    "            least_violences['violence'].append(data[game]['violence'])\n",
    "    \n",
    "    # Calculate averages for both groups\n",
    "    averages = {\n",
    "        'Score': ['neg', 'neu', 'pos', 'compound', \"violence\"],\n",
    "        'Average Most Violent': [\n",
    "            average(most_violences['neg']),\n",
    "            average(most_violences['neu']),\n",
    "            average(most_violences['pos']),\n",
    "            average(most_violences['compound']),\n",
    "            average(most_violences['violence'])\n",
    "        ],\n",
    "        'Average Least Violent': [\n",
    "            average(least_violences['neg']),\n",
    "            average(least_violences['neu']),\n",
    "            average(least_violences['pos']),\n",
    "            average(least_violences['compound']),\n",
    "            average(least_violences['violence'])\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame to display the table\n",
    "    df = pd.DataFrame(averages)\n",
    "    \n",
    "    # Calculate and add a column for the difference between the most and least violent scores\n",
    "    df['Difference (Most - Least)'] = df['Average Most Violent'] - df['Average Least Violent']\n",
    "    \n",
    "    return df\n",
    "\n",
    "most_violent = [\"Doom (2016 video game)\", \"Grand Theft Auto V\", \"Mortal Kombat (1992 video game)\", \"God of War (2005 video game)\", \"Manhunt (video game)\", \"Gears of War (video game)\", \"Call of Duty 4: Modern Warfare\", \"Dead Space (2008 video game)\", \"Resident Evil (1996 video game)\", \"Hotline Miami\"]\n",
    "least_violent = [\"Animal Crossing: New Horizons\", \"Stardew Valley\", \"The Sims 4\", \"Minecraft\", \"Tetris\", \"Monument Valley (video game)\", \"Super Mario Odyssey\", \"Journey (2012 video game)\", \"Katamari Damacy\", \"Fez (video game)\"]\n",
    "compare_violence_scores(sentiment_filtered_by_length, most_violent, least_violent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = get_or_create_value(lambda: {}, \"game_data_with_vader_sentiment.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting production year from category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for game, data in game_data.items():\n",
    "    match = re.match(r'^\\d+', data[\"category\"])\n",
    "    if match:\n",
    "        data[\"year\"] = int(match.group())\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "    data.pop(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def calculate_sentiment_statistics(game_data: dict, sentiment_key: str):\n",
    "    \"\"\"\n",
    "    Calculate the average and standard deviation of specified sentiment scores per year from a dataset\n",
    "    where each entry is a dictionary containing a year and a nested 'sentiment' dictionary with sentiment scores.\n",
    "\n",
    "    :param game_data: Dictionary of dictionaries with 'year' and nested 'sentiment' dictionary.\n",
    "    :param sentiment_key: Key for the sentiment score to be calculated (e.g., 'violence', 'compound').\n",
    "    :return: Two dictionaries with years as keys and average scores and standard deviations as values.\n",
    "    \"\"\"\n",
    "    sentiment_sum_per_year = defaultdict(float)\n",
    "    count_per_year = defaultdict(int)\n",
    "\n",
    "    # Sum sentiment scores and count entries for each year\n",
    "    for game in game_data.values():\n",
    "        try:\n",
    "            year = game['year']\n",
    "            sentiment_score = game[\"sentiment\"][sentiment_key]\n",
    "            sentiment_sum_per_year[year] += sentiment_score\n",
    "            count_per_year[year] += 1\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in data: {e}\")\n",
    "\n",
    "    # Calculate the average sentiment score for each year\n",
    "    average_sentiment_per_year = {year: sentiment_sum_per_year[year] / count_per_year[year]\n",
    "                                  for year in sentiment_sum_per_year}\n",
    "\n",
    "    # Calculate standard deviation for each year\n",
    "    std_dev_per_year = {year: np.std([game[\"sentiment\"][sentiment_key] \n",
    "                                      for game in game_data.values() if game['year'] == year])\n",
    "                        for year in count_per_year}\n",
    "\n",
    "    return average_sentiment_per_year, std_dev_per_year\n",
    "\n",
    "\n",
    "\n",
    "def plot_violence_statistics(average_violence, std_deviation):\n",
    "    \"\"\"\n",
    "    Plot the violence statistics.\n",
    "\n",
    "    :param average_violence: Dictionary of average violence scores per year.\n",
    "    :param std_deviation: Dictionary of standard deviations per year.\n",
    "    \"\"\"\n",
    "    # Sort the data by year\n",
    "    sorted_years = sorted(average_violence.keys())\n",
    "    average_violences = [average_violence[year] for year in sorted_years]\n",
    "    std_devs = [std_deviation[year] for year in sorted_years]\n",
    "\n",
    "    # Plotting with enhancements\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot average violence scores with error bars\n",
    "    plt.errorbar(sorted_years, average_violences, yerr=std_devs, fmt='-o',\n",
    "                 label='Average with Std Dev', color='blue')\n",
    "\n",
    "    # Fit and plot a trend line\n",
    "    z = np.polyfit(sorted_years, average_violences, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(sorted_years, p(sorted_years), \"r--\", label='Trend Line')\n",
    "\n",
    "    # Labels and title\n",
    "    plt.title('Violence Scores Over the Years with Trend and Variability')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Violence Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "average_violence, std_deviation = calculate_sentiment_statistics(game_data, \"violence\")\n",
    "plot_violence_statistics(average_violence, std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Assuming game_data is a dictionary where the key is the game name and the value is another dictionary\n",
    "# that includes 'country of development' and 'violence' score within a 'sentiment' sub-dictionary.\n",
    "\n",
    "# Step 1: Extract relevant data\n",
    "country_violence_data = defaultdict(list)\n",
    "for game, details in game_data.items():\n",
    "    # We assume 'violence' scores are within a 'sentiment' sub-dictionary\n",
    "    violence_score = details[\"sentiment\"][\"violence\"]\n",
    "    for country in details[\"country of development\"]:\n",
    "        country_violence_data[country].append(violence_score)\n",
    "\n",
    "# Step 2: Count the number of games per country\n",
    "game_counts = Counter({country: len(scores) for country, scores in country_violence_data.items()})\n",
    "\n",
    "# Step 3: Calculate average violence score for each country with at least 100 games or is Denmark\n",
    "average_violence_scores = {}\n",
    "for country, violence_scores in country_violence_data.items():\n",
    "    if (game_counts[country] >= 100 or country == \"Denmark\") and country != \"other\":\n",
    "        average_violence_scores[country] = sum(violence_scores) / len(violence_scores)\n",
    "\n",
    "# Step 4: Add all other countries to 'Others'\n",
    "other_violence_scores = []\n",
    "for country, violence_scores in country_violence_data.items():\n",
    "    if (game_counts[country] < 100 and country != \"Denmark\") or \"other\":\n",
    "        other_violence_scores.extend(violence_scores)\n",
    "if other_violence_scores:\n",
    "    average_violence_scores[\"Others\"] = sum(other_violence_scores) / len(other_violence_scores)\n",
    "\n",
    "\n",
    "# Calculate overall average violence score\n",
    "all_scores = [score for scores in country_violence_data.values() for score in scores]\n",
    "overall_average_violence = sum(all_scores) / len(all_scores)\n",
    "\n",
    "# Step 5: Plot the average violence scores\n",
    "countries = sorted(list(average_violence_scores.keys()), key= lambda x: average_violence_scores[x]) \n",
    "averages = [average_violence_scores[country] for country in countries]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(countries, averages)\n",
    "plt.xlabel('Country of Development')\n",
    "plt.ylabel('Average Violence Score')\n",
    "plt.title('Average Violence Score by Country of Development')\n",
    "plt.xticks(rotation=90)  # Rotate country names for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of tick-labels\n",
    "\n",
    "# Add a line for the overall average violence level\n",
    "plt.axhline(y=overall_average_violence, color='r', linestyle='-', label=f'Overall Average ({overall_average_violence:.2f})')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
