{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format and emojis: âœ… for solution, ðŸ“Š for graph captions, ðŸ’¬ for comments\n",
    "\n",
    "Copy these ones so that we all use the same ones (Windows 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function lets you print markdown with python code block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/SocialComplexityLab/socialgraphs2021/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc.), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday October the 31st, 2021 at 23:55. Hand in your IPython notebook file (with extension `.ipynb`) via http://peergrade.io/\n",
    "\n",
    "\n",
    "(If you haven't set up an account on peergrade yet, go to www.peergrade.io/join and type in the class code: ***DPZEV6***.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This year's Assignment 2 is all about analyzing the network of rappers.\n",
    "\n",
    "Note that this time I'm doing the exercises slightly differently in order to clean things up a bit. The issue is that the weekly lectures/exercises include quite a few instructions and intermediate results that are not quite something you guys can meaningfully answer. \n",
    "\n",
    "Therefore, in the assignment below, I have tried to reformulate the questions from the weekly exercises into something that is (hopefully) easier to answer. *Then I also note which lectures each question comes from*; that way, you can easily go back and find additional tips & tricks on how to solve things ðŸ˜‡\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To create our network, we downloaded the rapper Wiki pages from each coast (during Week 4) and linked them via the hyperlinks connecting pages to each other. To achieve this goal we have used regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain the strategy you have used to extract the hyperlinks from the Wiki-pages, assuming that you have already collected the rapper pages with the Wikipedia API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ… Firstly, we iterate over all the rapper text files and we add them as nodes in the network, while also including some attributes like `Coast` and `Content Length`. After that, we iterate again over all of the same files, doing the following:\n",
    "> * We search for **all** Wikipedia internal links that follow a specific format (we do that with a regular expression, explained in the next section). \n",
    "> * Some times these links include more than one wording (separated by '|'), but we are only interested in the first one.\n",
    "> * Then, we check if the resulting word or phrase is in the collection of rapper names we were given at the start of Week 4.\n",
    "> * If it is, then we connect the rapper on which file we are iterating over, edge origin, to the rapper in the Wikipedia internal link, edge destination (the direction is important)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the regular expressions you have built and explain in details how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ… The expression is the following `\\[\\[(.*?)\\]\\]`, which essentially matches everything between double square brackets. In particular:\n",
    "> * The square brackets `\\[ \\]` are *escaped* with a backlash (`\\`) just so they are to matched literally.\n",
    "> * The parenthesis `( )` form a group (although as it's only one group in the whole expression that we don't need to match as repeating, the parenthesis can be removed).\n",
    "> * `.*?`: This is a part of the pattern enclosed in parentheses and it's the interesting part. It uses the following elements:\n",
    ">   * `.`: This period (dot) is a metacharacter in regular expressions that matches any character except for a newline character.\n",
    ">   * `*`: This asterisk is another metacharacter and it means \"zero or more occurrences of the preceding element.\" So, `.*` means zero or more occurrences of any character.\n",
    ">   * `?`: The question mark is another metacharacter, and when used after `*`, it makes the `*` operator non-greedy, meaning it matches as few characters as possible. Without the `?`, `.*` would be greedy and match as many characters as possible, so from the first link's `[[`, to the last link's `]]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Network visualization and basic stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your network of rappers (from lecture 5) and calculate stats (from lecture 4 and 5). For this exercise, we assume that you have already generated the network and extracted the largest weakly connected component (the \"largest weakly connected component\" of a directed network is the subgraph consisting of the nodes that would constitute the largest connected component if the network were undirected) . The visualization and statistics should be done for the largest weakly connected component only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load graph object from pickle file that we saved in lecture 4\n",
    "G = pickle.load(open('./Week4_graph.pickle', 'rb'))\n",
    "SEED = 1234\n",
    "\n",
    "# Extract the largest weakly connected component\n",
    "import networkx as nx\n",
    "\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "DIRECTED_GRAPH = G.copy() # directed graph\n",
    "UNDIRECTED_GRAPH = G.to_undirected() #undirected graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Exercise 1a_: Stats (see lecture 4 and 5 for more hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the number of nodes in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> âœ… Number of nodes: 727"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(f\"> âœ… Number of nodes: {len(G.nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the number of links?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> âœ… Number of edges: 5803"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(f\"> âœ… Number of edges: {len(G.edges)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Who is the top connected rapper? (Report results for the in-degrees and out-degrees). Comment on your findings. Is this what you would have expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> âœ… The most in-connected rapper is 'Snoop Dogg' with 127 in-degree and the most out-connected rapper is 'Drag-On' with 52 out-degree."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = DIRECTED_GRAPH\n",
    "most_connected_in = sorted(dict(G.in_degree).items(), key=lambda x:x[1], reverse=True)[0]\n",
    "most_connected_out = sorted(dict(G.out_degree).items(), key=lambda x:x[1], reverse=True)[0]\n",
    "printmd(f\"> âœ… The most in-connected rapper is '{most_connected_in[0]}' with {most_connected_in[1]} in-degree and \" +\n",
    "        f\"the most out-connected rapper is '{most_connected_out[0]}' with {most_connected_out[1]} out-degree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Who are the top 5 most connected east-coast rappers (again in terms of in/out-degree)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Who are the top 5 most connected west-coast rappers (again in terms of in/out-degree)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the in- and out-degree distributions for the whole network. \n",
    "   * Use axes that make sense for visualizing this particular distribution.\n",
    "   * What do you observe? \n",
    "   * Give a pedagogical explaination of why the in-degree distribution is different from the out-degree distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the exponent (by using the `powerlaw` package) for the in- and out-degree distributions. What does it say about our network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the two degree distributions two the degree distribution of a *random network* (undirected) with the same number of nodes and probability of connection *p*. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Exercise 1b_: Visualization (see lecture 5 for more hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a nice visualization of the total (directed) network:\n",
    "   * Color nodes according to the role;\n",
    "   * Scale node-size according to degree;\n",
    "   * Get node positions based on either the Force Atlas 2 algorithm, or the built-in algorithms for networkX;\n",
    "   * Whatever else you feel like that would make the visualization nicer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Describe the structure you observe. What useful information can you decipher from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word-clouds\n",
    "\n",
    "Create your own version of the word-clouds (from lecture 7). For this exercise we assume you know how to download and clean text from rappers' Wikipedia pages.\n",
    "\n",
    "Here's what you need to do:\n",
    "> * Create a word-cloud for each coast according to the novel TF-TR method. Feel free to make it as fancy as you like. Explain your process and comment on your results.\n",
    "> * For each coast, what are the 5 words with the highest TR scores? Comment on your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Communities\n",
    "\n",
    "Find communities and their modularity (from lecture 7).\n",
    "\n",
    "Here's what you need to do:\n",
    "> * In your own words, explain what the measure \"modularity\" is, and the intuition behind the formula you use to compute it. \n",
    "> * Find communities in the network, and explain how you chose to identify the communities: Which algorithm did you use and how does it work?\n",
    "> * Comment on your results:\n",
    ">   * How many communities did you find in total?\n",
    ">   * Compute the value of modularity with the partition created by the algorithm.\n",
    ">   * Plot and/or print the distribution of community sizes (whichever makes most sense). Comment on your result.\n",
    "> * Now, partition your rappers into two communities based on which coast they represent.\n",
    ">   * What is the modularity of this partition? Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Sentiment of communities\n",
    "\n",
    "Analyze the sentiment of communities (lecture 8). More tips & tricks can be found, if you take a look at Lecture 8's exercises.\n",
    "\n",
    "A couple of additional instructions you will need below:\n",
    "* Average the average sentiment of the nodes in each community to find a community-level sentiment.\n",
    "\n",
    "Here's what you need to do (use the LabMT wordlist approach):\n",
    "> * Calculate and store sentiment for every rapper\n",
    "> * Create a histogram of all rappers' associated sentiments.\n",
    "> * What are the 10 rappers with happiest and saddest pages?\n",
    "\n",
    "Now, compute the sentiment of each coast: \n",
    "> * Which is the happiest and which is saddest coast according to the LabMT wordlist approach? (Take the coast's sentiment to be the average sentiment of the coast's rappers' pages (disregarding any rappers with sentiment 0).\n",
    "> * Use the \"label shuffling test\" (Week 5 and 8) to test if the coast with the highest wikipedia page sentiment has a page sentiment that is significantly higher (5% confidence bound) than a randomly selected group of rappers of the same size.\n",
    "> * Does the result make sense to you? Elaborate.\n",
    "\n",
    "**Congratulations for making it to the end of the Assignment. Good luck with your independent project**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
